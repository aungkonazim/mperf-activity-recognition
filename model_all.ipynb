{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,Input,MaxPooling1D,Flatten,Dense,Input,Activation,GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import Parallel,delayed\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Starting for no. of training users =  338\n",
      "Training length minutes =  10\n",
      "(10140, 500, 3)\n",
      "0.21153846153846154,iteration =  0\n",
      "(10140, 500, 3)\n",
      "0.21597633136094674,iteration =  1\n",
      "(10140, 500, 3)\n",
      "0.21745562130177515,iteration =  2\n",
      "10 --Done\n",
      "Training length minutes =  20\n",
      "(20280, 500, 3)\n",
      "0.21449704142011836,iteration =  0\n",
      "(20280, 500, 3)\n",
      "0.22140039447731755,iteration =  1\n",
      "(20280, 500, 3)\n",
      "0.22879684418145957,iteration =  2\n",
      "20 --Done\n",
      "Training length minutes =  30\n",
      "(30420, 500, 3)\n",
      "0.26528599605522685,iteration =  0\n",
      "(30420, 500, 3)\n",
      "0.27120315581854043,iteration =  1\n",
      "(30420, 500, 3)\n",
      "0.27087442472057854,iteration =  2\n",
      "30 --Done\n",
      "Training length minutes =  40\n",
      "(40560, 500, 3)\n",
      "0.27921597633136097,iteration =  0\n",
      "(40560, 500, 3)\n",
      "0.2750246548323471,iteration =  1\n",
      "(40560, 500, 3)\n",
      "0.27366863905325445,iteration =  2\n",
      "40 --Done\n",
      "Training length minutes =  50\n",
      "(50700, 500, 3)\n",
      "0.31715976331360946,iteration =  0\n",
      "(50700, 500, 3)\n",
      "0.32041420118343195,iteration =  1\n",
      "(50700, 500, 3)\n",
      "0.30029585798816566,iteration =  2\n",
      "50 --Done\n",
      "Training length minutes =  60\n",
      "(60840, 500, 3)\n",
      "0.33308678500986194,iteration =  0\n",
      "(60840, 500, 3)\n",
      "0.3319362261669954,iteration =  1\n",
      "(60840, 500, 3)\n",
      "0.334155161078238,iteration =  2\n",
      "60 --Done\n",
      "Training length minutes =  90\n",
      "(91260, 500, 3)\n",
      "0.40855796625027396,iteration =  0\n",
      "(91260, 500, 3)\n",
      "0.4198991891299584,iteration =  1\n",
      "(91260, 500, 3)\n",
      "0.39765505150120534,iteration =  2\n",
      "90 --Done\n",
      "Training length minutes =  120\n",
      "(121680, 500, 3)\n",
      "0.4472386587771203,iteration =  0\n",
      "(121680, 500, 3)\n",
      "0.4538132807363577,iteration =  1\n",
      "(121680, 500, 3)\n",
      "0.4451429980276134,iteration =  2\n",
      "120 --Done\n",
      "Training length minutes =  150\n",
      "(152100, 500, 3)\n",
      "0.47633136094674555,iteration =  0\n",
      "(152100, 500, 3)\n",
      "0.477284681130835,iteration =  1\n",
      "(152100, 500, 3)\n",
      "0.4747205785667324,iteration =  2\n",
      "150 --Done\n",
      "Training length minutes =  180\n",
      "(182520, 500, 3)\n",
      "0.49569910146833224,iteration =  0\n",
      "(182520, 500, 3)\n",
      "0.5033147052377822,iteration =  1\n",
      "(182520, 500, 3)\n",
      "0.4995342976112207,iteration =  2\n",
      "180 --Done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def get_participants_df(directory,window_size,min_length):\n",
    "    df = []\n",
    "    n = 60//window_size\n",
    "    for f in os.listdir(directory):\n",
    "        data = pickle.load(open(directory+f,'rb'))\n",
    "        df.append([f,data.shape[0]//n])\n",
    "    df = pd.DataFrame(df,columns=['user','total_test_length'])\n",
    "    return df[df.total_test_length>=min_length]\n",
    "\n",
    "def get_training_data(directory,\n",
    "                      train_length,\n",
    "                      n_user,\n",
    "                      participant_df,\n",
    "                      window_size):\n",
    "    n = 60//window_size\n",
    "    users = participant_df['user'].values[:n_user]\n",
    "    X = []\n",
    "    y = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        i = np.random.randint(0,df.shape[0]-n*train_length)\n",
    "        df = df[i:i+n*train_length]\n",
    "#         df = df.sample(n*train_length,replace=False)\n",
    "        X.append(np.concatenate(list(df['data'])))\n",
    "        y.extend([f]*df.shape[0])\n",
    "    y_dict = {a:i for i,a in enumerate(np.unique(y))}\n",
    "    y  = [y_dict[a] for a in y]\n",
    "    return np.concatenate(X),np.array(y),y_dict\n",
    "\n",
    "def get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,filepath):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model =  get_model(input_shape=(n_timesteps,n_channels),n_classes=n_classes)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max',save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_acc', mode='max', verbose=0,patience=40)\n",
    "    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=0,factor=0.5)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=300,verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "    print(accuracy_score(val_y,model.predict(val_x).argmax(axis=1)),end=',')\n",
    "    return model\n",
    "\n",
    "def get_model(input_shape=(500,3),n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(128,2,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(GRU(128,return_sequences=False,activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(350,name='feature'))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "window_size = 20\n",
    "activity = 'std'\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/'\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'\n",
    "min_test_total_length  = 100\n",
    "fs = 25\n",
    "n_timesteps,n_channels = fs*window_size,3 \n",
    "if not os.path.isdir(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "participant_df = get_participants_df(data_directory+'testing/',window_size,min_test_total_length)\n",
    "# n_users = list(np.arange(50,participant_df.shape[0],50))+[participant_df.shape[0]]\n",
    "train_lengths = list(np.arange(10,60,10))+list(np.arange(60,210,30))\n",
    "n_iters = np.arange(3)\n",
    "n_users = [participant_df.shape[0]]\n",
    "# train_lengths = [120]\n",
    "# n_iters = [1]\n",
    "\n",
    "for n_user in n_users[::-1]:\n",
    "    if not os.path.isdir(model_directory+str(n_user)):\n",
    "        os.makedirs(model_directory+str(n_user))\n",
    "    print('--'*30)\n",
    "    print('Starting for no. of training users = ',n_user)\n",
    "    for train_length in train_lengths:\n",
    "        print('Training length minutes = ',train_length)\n",
    "        if not os.path.isdir(model_directory+str(n_user)+'/'+str(train_length)):\n",
    "            os.makedirs(model_directory+str(n_user)+'/'+str(train_length))\n",
    "        for n_iter in n_iters:\n",
    "            X_train,y_train,user_dict = get_training_data(directory = data_directory+'training/',\n",
    "                                                          train_length=train_length,\n",
    "                                                          n_user=n_user,\n",
    "                                                          participant_df=participant_df,\n",
    "                                                          window_size=window_size) \n",
    "            print(X_train.shape)\n",
    "            pickle.dump(user_dict,open(model_directory+str(n_user)+'/'+str(train_length)+'/userdict_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.p','wb'))\n",
    "            model = get_model(input_shape=(n_timesteps,n_channels),n_classes=len(np.unique(y_train)))\n",
    "            model_filepath = model_directory+str(n_user)+'/'+str(train_length)+'/trainedmodel_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.hdf5'\n",
    "            model = get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,model_filepath)\n",
    "            print('iteration = ',n_iter)\n",
    "        print(train_length, '--Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "window_size = 20\n",
    "train_length = 120\n",
    "activity = 'walking'\n",
    "n_user  = 333\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'+str(n_user)+'/'+str(train_length)+'/'\n",
    "\n",
    "model_directory = './models/20/walking/333/120/trained_model_seed_100_iteration_1.hdf5'\n",
    "# model = tf.keras.models.load_model(model_directory+'trained_model_seed_100_iteration_0.hdf5')\n",
    "model = tf.keras.models.load_model(model_directory)\n",
    "\n",
    "# user_dict = pickle.load(open(model_directory+'user_dict_seed_100_iteration_0.p','rb'))\n",
    "\n",
    "user_dict = pickle.load(open('./models/20/walking/333/120/user_dict_seed_100_iteration_1.p','rb'))\n",
    "\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/testing/'\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "def get_training_data(directory,\n",
    "                      test_length,\n",
    "                      user_dict,\n",
    "                      window_size,\n",
    "                      model):\n",
    "    n = 60//window_size\n",
    "    users = list(user_dict.keys())\n",
    "    results = []\n",
    "    y_orig = []\n",
    "    y_pred = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        X = np.concatenate(list(df['data']))\n",
    "        pred = model.predict(X).argmax(axis=1)\n",
    "        results.append(accuracy_score([user_dict[f]]*df.shape[0],pred))\n",
    "        y_orig.extend([user_dict[f]]*df.shape[0])\n",
    "        y_pred.extend(list(pred))\n",
    "    return results,pd.DataFrame({'original':y_orig,'prediction':y_pred})\n",
    "\n",
    "results,df = get_training_data(data_directory,1,user_dict,window_size,model)\n",
    "\n",
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df['original'],df['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('original',as_index=False).apply(lambda a:pd.Series({'accuracy':accuracy_score(a['original'],a['prediction'])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
