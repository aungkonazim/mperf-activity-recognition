{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,Input,MaxPooling1D,Flatten,Dense,Input,Activation,GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import Parallel,delayed\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22]\n",
      "------------------------------------------------------------\n",
      "Starting for no. of training users =  22\n",
      "Training length minutes =  -1\n",
      "(20949, 500, 3)\n",
      "0.6410501193317423,iteration =  0\n",
      "(20949, 500, 3)\n",
      "0.6343675417661098,iteration =  1\n",
      "(20949, 500, 3)\n",
      "0.6257756563245823,iteration =  2\n",
      "-1 --Done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def get_participants_df(directory,window_size,min_length):\n",
    "    df = []\n",
    "    n = 60//window_size\n",
    "    for f in os.listdir(directory):\n",
    "        if f[0]=='.':\n",
    "            continue\n",
    "        data = pickle.load(open(directory+f,'rb'))\n",
    "        df.append([f,data.shape[0]//n])\n",
    "    df = pd.DataFrame(df,columns=['user','total_test_length'])\n",
    "    return df[df.total_test_length>=min_length]\n",
    "\n",
    "def get_training_data(directory,\n",
    "                      train_length,\n",
    "                      n_user,\n",
    "                      participant_df,\n",
    "                      window_size):\n",
    "    n = 60//window_size\n",
    "    users = participant_df['user'].values[:n_user]\n",
    "    X = []\n",
    "    y = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        if train_length==-1 or df.shape[0]<=n*train_length:\n",
    "            X.append(np.concatenate(list(df['data'])))\n",
    "            y.extend([f]*df.shape[0])\n",
    "        else:\n",
    "            i = np.random.randint(0,df.shape[0]-n*train_length)\n",
    "            df = df[i:i+n*train_length]\n",
    "    #         df = df.sample(n*train_length,replace=False)\n",
    "            X.append(np.concatenate(list(df['data'])))\n",
    "            y.extend([f]*df.shape[0])\n",
    "    y_dict = {a:i for i,a in enumerate(np.unique(y))}\n",
    "    y  = [y_dict[a] for a in y]\n",
    "    return np.concatenate(X),np.array(y),y_dict\n",
    "\n",
    "def get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,filepath):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model =  get_model(input_shape=(n_timesteps,n_channels),n_classes=n_classes)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min',save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0,patience=40)\n",
    "    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=0,factor=0.5)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=100,verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "    print(accuracy_score(val_y,model.predict(val_x).argmax(axis=1)),end=',')\n",
    "    return model\n",
    "\n",
    "def get_model(input_shape=(500,3),n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(128,2,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(GRU(128,return_sequences=False,activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(350,name='feature'))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "window_size = 20\n",
    "activity = 'walking_moral'\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/'\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'\n",
    "min_test_total_length  = 10\n",
    "fs = 25\n",
    "n_timesteps,n_channels = fs*window_size,3 \n",
    "if not os.path.isdir(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "participant_df = get_participants_df(data_directory+'testing/',window_size,min_test_total_length)\n",
    "# n_users = list(np.arange(50,participant_df.shape[0],50))+[participant_df.shape[0]]\n",
    "# train_lengths = list(np.arange(10,60,10))+list(np.arange(60,210,30))\n",
    "# train_lengths = np.arange(90,210,30)\n",
    "n_iters = np.arange(0,3,1)\n",
    "n_users = [participant_df.shape[0]]\n",
    "train_lengths = [-1]\n",
    "# print(n_users)\n",
    "# train_lengths = [10,30,60,150,300]\n",
    "# train_lengths = [270]\n",
    "# n_users = np.arange(50,350,50)\n",
    "# n_users = [10,20,40]+list(n_users)\n",
    "# n_iters = [1]\n",
    "print(n_users)\n",
    "for n_user in n_users:\n",
    "    if not os.path.isdir(model_directory+str(n_user)):\n",
    "        os.makedirs(model_directory+str(n_user))\n",
    "    print('--'*30)\n",
    "    print('Starting for no. of training users = ',n_user)\n",
    "    for train_length in train_lengths:\n",
    "        print('Training length minutes = ',train_length)\n",
    "        if not os.path.isdir(model_directory+str(n_user)+'/'+str(train_length)):\n",
    "            os.makedirs(model_directory+str(n_user)+'/'+str(train_length))\n",
    "        for n_iter in n_iters:\n",
    "            X_train,y_train,user_dict = get_training_data(directory = data_directory+'training/',\n",
    "                                                          train_length=train_length,\n",
    "                                                          n_user=n_user,\n",
    "                                                          participant_df=participant_df,\n",
    "                                                          window_size=window_size) \n",
    "            print(X_train.shape)\n",
    "            pickle.dump(user_dict,open(model_directory+str(n_user)+'/'+str(train_length)+'/userdict_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.p','wb'))\n",
    "            model = get_model(input_shape=(n_timesteps,n_channels),n_classes=len(np.unique(y_train)))\n",
    "            model_filepath = model_directory+str(n_user)+'/'+str(train_length)+'/trainedmodel_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.hdf5'\n",
    "            model = get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,model_filepath)\n",
    "            print('iteration = ',n_iter)\n",
    "        print(train_length, '--Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_testing_data(directory,min_length,window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    n = 60//window_size\n",
    "    for f in os.listdir(directory):\n",
    "        if f[0]=='.':\n",
    "            continue\n",
    "        data = pickle.load(open(directory+f,'rb'))\n",
    "        if data.shape[0]//n<min_length:\n",
    "            continue\n",
    "        X.append(np.concatenate(list(data['data'])))\n",
    "        y.extend([f]*data.shape[0])\n",
    "    return np.concatenate(X),np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "def get_test_results_alll(window_size=20,activity='sports',n_user=50,min_length=100):\n",
    "    data_directory = './data/'+str(window_size)+'/'+activity+'/testing/'\n",
    "    model_directory = './models/'+str(window_size)+'/'+activity+'/'+str(n_user)+'/'\n",
    "    train_lengths = os.listdir(model_directory)\n",
    "    X,y = get_testing_data(data_directory,min_length,window_size)\n",
    "    save_directory = './predictions/'+str(window_size)+'/'+activity+'/'+str(n_user)+'/'\n",
    "    result_directory = './results/'\n",
    "    if not os.path.isdir(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    X.shape,y.shape\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    def get_dictfilename(a):\n",
    "        a = a.replace('trainedmodel','userdict').replace('hdf5','p')\n",
    "        return a\n",
    "\n",
    "    def get_predictions(df):\n",
    "        indexes = np.array(list(df['index']))\n",
    "        y_pred = model.predict(X[indexes])\n",
    "        df['y_prob'] = list(y_pred)\n",
    "        df['y_pred'] = y_pred.argmax(axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "    for f in list(os.listdir(model_directory)):\n",
    "        if f in os.listdir(save_directory) or f[0]=='.':\n",
    "            continue\n",
    "        print(f)\n",
    "        model_files = [model_directory+f+'/'+a for a in os.listdir(model_directory+f) if a[-1]=='5']\n",
    "        dict_files = [get_dictfilename(a) for a in model_files]\n",
    "        pairs = list(zip(model_files,dict_files))\n",
    "        predictions_all = []\n",
    "        for i,a in enumerate(pairs):\n",
    "            print(i,end=',')\n",
    "            m_name,d_name = a\n",
    "            user_dict = pickle.load(open(d_name,'rb'))\n",
    "            indexes = np.array([i for i in range(len(y)) if y[i] in user_dict.keys()])\n",
    "            y_temp = y[indexes]\n",
    "            X_temp = X[indexes]\n",
    "            y_final = np.array([user_dict[a] for a in y_temp])\n",
    "            index_df = pd.DataFrame({'user':y_temp,'y':y_final,'index':np.arange(len(y_temp))})\n",
    "            model = tf.keras.models.load_model(m_name)\n",
    "            predictions = index_df.groupby('user',as_index=False).apply(get_predictions)\n",
    "            predictions['iteration'] = i\n",
    "            predictions_all.append(predictions)\n",
    "        predictions_all = pd.concat(predictions_all)\n",
    "        pickle.dump(predictions_all,open(save_directory+f,'wb'))\n",
    "        print(f,'done')\n",
    "\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from scipy.stats import mode\n",
    "\n",
    "    def get_results(df):\n",
    "        df['y_prob'] = df['y_prob'].apply(lambda a:a.reshape(1,-1))\n",
    "        rows = []\n",
    "        rows.append([0,accuracy_score(df['y'],df['y_pred']),accuracy_score(df['y'],df['y_pred']),np.int64(f),df['user'].values[0],df['iteration'].values[0]])\n",
    "        for t in test_lengths:\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            y_pred_maxmean = []\n",
    "            for j in range(n_iter):\n",
    "                n = t*3\n",
    "                if n>df.shape[0]:\n",
    "                    continue\n",
    "                temp_df = df.sample(n,replace=False)\n",
    "                y_true.append(temp_df['y'].values[0])\n",
    "                y_pred.append(mode(temp_df['y_pred'])[0][0])\n",
    "                y_pred_maxmean.append(np.concatenate(list(temp_df['y_prob'])).mean(axis=0).argmax())\n",
    "            rows.append([t,accuracy_score(y_true,y_pred),accuracy_score(y_true,y_pred_maxmean),np.int64(f),df['user'].values[0],df['iteration'].values[0]])\n",
    "        return pd.DataFrame(rows,columns=['test_length','majority_score',\n",
    "                                          'maxmean_score','train_length',\n",
    "                                          'user','iteration'])\n",
    "\n",
    "\n",
    "    import sys\n",
    "    from joblib import Parallel,delayed\n",
    "    # if activity in ['stationery','std5']:\n",
    "    test_lengths = list(np.arange(1,10,1))+list(np.arange(10,130,10))+list(np.arange(150,750,50))\n",
    "    # else:\n",
    "    #     test_lengths = list(np.arange(1,10,1))+list(np.arange(10,60,5))\n",
    "    n_iter = 100\n",
    "    base_window_size = 20\n",
    "    final_results = []\n",
    "    train_lengths = []\n",
    "    # if activity+'.p' in os.listdir(result_directory):\n",
    "    #     df = pickle.load(open(result_directory+activity+'.p','rb'))\n",
    "    #     final_results.append(df)\n",
    "    #     train_lengths = [str(a) for a in df['train_length'].unique()]\n",
    "\n",
    "    for f in os.listdir(save_directory):\n",
    "        if f in train_lengths:\n",
    "            continue\n",
    "        print(f)\n",
    "        dd = pickle.load(open(save_directory+f,'rb'))\n",
    "        all_dfs = list(dd.groupby(['user','iteration'],as_index=False))\n",
    "        all_results = Parallel(n_jobs=40,verbose=2)(delayed(get_results)(all_dfs[k][1]) for k in range(len(all_dfs)))\n",
    "        results = pd.concat(all_results)\n",
    "        final_results.append(results)\n",
    "        pickle.dump(pd.concat(final_results),open(result_directory+activity+str(n_user)+'.p','wb'))\n",
    "        print(f,'done')\n",
    "\n",
    "    results = pickle.load(open(result_directory+activity+str(n_user)+'.p','rb'))\n",
    "\n",
    "    final_results = results.groupby(['test_length','iteration','train_length'],as_index=False).mean().groupby(['test_length','train_length'],as_index=False).mean()\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.rcParams.update({'font.size':40})\n",
    "    plt.figure(figsize=(30,20))\n",
    "    sns.lineplot(x='test_length',y='majority_score',hue='train_length',data=final_results)\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    plt.rcParams.update({'font.size':40})\n",
    "    plt.figure(figsize=(30,20))\n",
    "    sns.lineplot(x='test_length',y='maxmean_score',hue='train_length',data=final_results)\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    def save_data_final(extra=''):\n",
    "        final_result_directory = './final_results/'\n",
    "        activity1 = activity\n",
    "        if activity=='std':\n",
    "            activity1 += '20'\n",
    "        activity1+=extra\n",
    "        maxmean = pd.pivot_table(final_results,columns='train_length',index='test_length',values='maxmean_score',aggfunc='mean')\n",
    "        maxmean.to_csv(final_result_directory+activity1+'_maxmean.csv')\n",
    "        majority = pd.pivot_table(final_results,columns='train_length',index='test_length',values='majority_score',aggfunc='mean')\n",
    "        majority.to_csv(final_result_directory+activity1+'_majority.csv')\n",
    "\n",
    "    save_data_final('_'+str(n_user))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = np.arange(50,350,50)\n",
    "n_users = [10,20,40]+list(n_users)\n",
    "for n_user in n_users:\n",
    "    get_test_results_alll(window_size=20,activity='sports',n_user=n_user,min_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def get_participants_df(directory,window_size,min_length):\n",
    "    df = []\n",
    "    n = 60//window_size\n",
    "    for f in os.listdir(directory):\n",
    "        data = pickle.load(open(directory+f,'rb'))\n",
    "        df.append([f,data.shape[0]//n])\n",
    "    df = pd.DataFrame(df,columns=['user','total_test_length'])\n",
    "    return df[df.total_test_length>=min_length]\n",
    "\n",
    "def get_training_data(directory,\n",
    "                      train_length,\n",
    "                      n_user,\n",
    "                      participant_df,\n",
    "                      window_size):\n",
    "    n = 60//window_size\n",
    "    users = participant_df['user'].values[:n_user]\n",
    "    X = []\n",
    "    y = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        i = np.random.randint(0,df.shape[0]-n*train_length)\n",
    "        df = df[i:i+n*train_length]\n",
    "#         df = df.sample(n*train_length,replace=False)\n",
    "        X.append(np.concatenate(list(df['data'])))\n",
    "        y.extend([f]*df.shape[0])\n",
    "    y_dict = {a:i for i,a in enumerate(np.unique(y))}\n",
    "    y  = [y_dict[a] for a in y]\n",
    "    return np.concatenate(X),np.array(y),y_dict\n",
    "\n",
    "def get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,filepath):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model =  get_model(input_shape=(n_timesteps,n_channels),n_classes=n_classes)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max',save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_acc', mode='max', verbose=0,patience=40)\n",
    "    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=0,factor=0.5)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=300,verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "    print(accuracy_score(val_y,model.predict(val_x).argmax(axis=1)),end=',')\n",
    "    return model\n",
    "\n",
    "def get_model(input_shape=(500,3),n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(128,2,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(GRU(128,return_sequences=False,activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(350,name='feature'))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "window_size = 20\n",
    "activity = 'sports'\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/'\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'\n",
    "min_test_total_length  = 100\n",
    "fs = 25\n",
    "n_timesteps,n_channels = fs*window_size,3 \n",
    "if not os.path.isdir(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "participant_df = get_participants_df(data_directory+'testing/',window_size,min_test_total_length)\n",
    "# n_users = list(np.arange(50,participant_df.shape[0],50))+[participant_df.shape[0]]\n",
    "train_lengths = list(np.arange(10,60,10))+list(np.arange(60,210,30))\n",
    "n_iters = np.arange(3)\n",
    "n_users = [participant_df.shape[0]]\n",
    "# train_lengths = [120]\n",
    "# n_iters = [1]\n",
    "\n",
    "for n_user in n_users[::-1]:\n",
    "    if not os.path.isdir(model_directory+str(n_user)):\n",
    "        os.makedirs(model_directory+str(n_user))\n",
    "    print('--'*30)\n",
    "    print('Starting for no. of training users = ',n_user)\n",
    "    for train_length in train_lengths:\n",
    "        print('Training length minutes = ',train_length)\n",
    "        if not os.path.isdir(model_directory+str(n_user)+'/'+str(train_length)):\n",
    "            os.makedirs(model_directory+str(n_user)+'/'+str(train_length))\n",
    "        for n_iter in n_iters:\n",
    "            X_train,y_train,user_dict = get_training_data(directory = data_directory+'training/',\n",
    "                                                          train_length=train_length,\n",
    "                                                          n_user=n_user,\n",
    "                                                          participant_df=participant_df,\n",
    "                                                          window_size=window_size) \n",
    "            print(X_train.shape)\n",
    "            pickle.dump(user_dict,open(model_directory+str(n_user)+'/'+str(train_length)+'/userdict_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.p','wb'))\n",
    "            model = get_model(input_shape=(n_timesteps,n_channels),n_classes=len(np.unique(y_train)))\n",
    "            model_filepath = model_directory+str(n_user)+'/'+str(train_length)+'/trainedmodel_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.hdf5'\n",
    "            model = get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,model_filepath)\n",
    "            print('iteration = ',n_iter)\n",
    "        print(train_length, '--Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "window_size = 20\n",
    "train_length = 120\n",
    "activity = 'walking'\n",
    "n_user  = 333\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'+str(n_user)+'/'+str(train_length)+'/'\n",
    "\n",
    "model_directory = './models/20/walking/333/120/trained_model_seed_100_iteration_1.hdf5'\n",
    "# model = tf.keras.models.load_model(model_directory+'trained_model_seed_100_iteration_0.hdf5')\n",
    "model = tf.keras.models.load_model(model_directory)\n",
    "\n",
    "# user_dict = pickle.load(open(model_directory+'user_dict_seed_100_iteration_0.p','rb'))\n",
    "\n",
    "user_dict = pickle.load(open('./models/20/walking/333/120/user_dict_seed_100_iteration_1.p','rb'))\n",
    "\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/testing/'\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "def get_training_data(directory,\n",
    "                      test_length,\n",
    "                      user_dict,\n",
    "                      window_size,\n",
    "                      model):\n",
    "    n = 60//window_size\n",
    "    users = list(user_dict.keys())\n",
    "    results = []\n",
    "    y_orig = []\n",
    "    y_pred = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        X = np.concatenate(list(df['data']))\n",
    "        pred = model.predict(X).argmax(axis=1)\n",
    "        results.append(accuracy_score([user_dict[f]]*df.shape[0],pred))\n",
    "        y_orig.extend([user_dict[f]]*df.shape[0])\n",
    "        y_pred.extend(list(pred))\n",
    "    return results,pd.DataFrame({'original':y_orig,'prediction':y_pred})\n",
    "\n",
    "results,df = get_training_data(data_directory,1,user_dict,window_size,model)\n",
    "\n",
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df['original'],df['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('original',as_index=False).apply(lambda a:pd.Series({'accuracy':accuracy_score(a['original'],a['prediction'])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('./models/','zip','./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
