{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cerebralcortex/kessel_jupyter_virtualenv/tensorflow/lib/python3.7/site-packages/pandas-1.2.3-py3.7-linux-x86_64.egg/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,Input,MaxPooling1D,Flatten,Dense,Input,Activation,GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import Parallel,delayed\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[346]\n",
      "------------------------------------------------------------\n",
      "Starting for no. of training users =  346\n",
      "Training length minutes =  210\n",
      "(217980, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def get_participants_df(directory,window_size,min_length):\n",
    "    df = []\n",
    "    n = 60//window_size\n",
    "    for f in os.listdir(directory):\n",
    "        if f[0]=='.':\n",
    "            continue\n",
    "        data = pickle.load(open(directory+f,'rb'))\n",
    "        df.append([f,data.shape[0]//n])\n",
    "    df = pd.DataFrame(df,columns=['user','total_test_length'])\n",
    "    return df[df.total_test_length>=min_length]\n",
    "\n",
    "def get_training_data(directory,\n",
    "                      train_length,\n",
    "                      n_user,\n",
    "                      participant_df,\n",
    "                      window_size):\n",
    "    n = 60//window_size\n",
    "    users = participant_df['user'].values[:n_user]\n",
    "    X = []\n",
    "    y = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        i = np.random.randint(0,df.shape[0]-n*train_length)\n",
    "        df = df[i:i+n*train_length]\n",
    "#         df = df.sample(n*train_length,replace=False)\n",
    "        X.append(np.concatenate(list(df['data'])))\n",
    "        y.extend([f]*df.shape[0])\n",
    "    y_dict = {a:i for i,a in enumerate(np.unique(y))}\n",
    "    y  = [y_dict[a] for a in y]\n",
    "    return np.concatenate(X),np.array(y),y_dict\n",
    "\n",
    "def get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,filepath):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model =  get_model(input_shape=(n_timesteps,n_channels),n_classes=n_classes)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max',save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_acc', mode='max', verbose=0,patience=40)\n",
    "    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=0,factor=0.5)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=300,verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "    print(accuracy_score(val_y,model.predict(val_x).argmax(axis=1)),end=',')\n",
    "    return model\n",
    "\n",
    "def get_model(input_shape=(500,3),n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(128,2,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(GRU(128,return_sequences=False,activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(350,name='feature'))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "window_size = 20\n",
    "activity = 'sports'\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/'\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'\n",
    "min_test_total_length  = 100\n",
    "fs = 25\n",
    "n_timesteps,n_channels = fs*window_size,3 \n",
    "if not os.path.isdir(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "participant_df = get_participants_df(data_directory+'testing/',window_size,min_test_total_length)\n",
    "# n_users = list(np.arange(50,participant_df.shape[0],50))+[participant_df.shape[0]]\n",
    "train_lengths = list(np.arange(10,60,10))+list(np.arange(60,450,30))\n",
    "n_iters = np.arange(3)\n",
    "n_users = [participant_df.shape[0]]\n",
    "train_lengths = np.arange(210,330,30)\n",
    "# n_iters = [1]\n",
    "print(n_users)\n",
    "for n_user in n_users[::-1]:\n",
    "    if not os.path.isdir(model_directory+str(n_user)):\n",
    "        os.makedirs(model_directory+str(n_user))\n",
    "    print('--'*30)\n",
    "    print('Starting for no. of training users = ',n_user)\n",
    "    for train_length in train_lengths:\n",
    "        print('Training length minutes = ',train_length)\n",
    "        if not os.path.isdir(model_directory+str(n_user)+'/'+str(train_length)):\n",
    "            os.makedirs(model_directory+str(n_user)+'/'+str(train_length))\n",
    "        for n_iter in n_iters:\n",
    "            X_train,y_train,user_dict = get_training_data(directory = data_directory+'training/',\n",
    "                                                          train_length=train_length,\n",
    "                                                          n_user=n_user,\n",
    "                                                          participant_df=participant_df,\n",
    "                                                          window_size=window_size) \n",
    "            print(X_train.shape)\n",
    "            pickle.dump(user_dict,open(model_directory+str(n_user)+'/'+str(train_length)+'/userdict_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.p','wb'))\n",
    "            model = get_model(input_shape=(n_timesteps,n_channels),n_classes=len(np.unique(y_train)))\n",
    "            model_filepath = model_directory+str(n_user)+'/'+str(train_length)+'/trainedmodel_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.hdf5'\n",
    "            model = get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,model_filepath)\n",
    "            print('iteration = ',n_iter)\n",
    "        print(train_length, '--Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def get_participants_df(directory,window_size,min_length):\n",
    "    df = []\n",
    "    n = 60//window_size\n",
    "    for f in os.listdir(directory):\n",
    "        data = pickle.load(open(directory+f,'rb'))\n",
    "        df.append([f,data.shape[0]//n])\n",
    "    df = pd.DataFrame(df,columns=['user','total_test_length'])\n",
    "    return df[df.total_test_length>=min_length]\n",
    "\n",
    "def get_training_data(directory,\n",
    "                      train_length,\n",
    "                      n_user,\n",
    "                      participant_df,\n",
    "                      window_size):\n",
    "    n = 60//window_size\n",
    "    users = participant_df['user'].values[:n_user]\n",
    "    X = []\n",
    "    y = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        i = np.random.randint(0,df.shape[0]-n*train_length)\n",
    "        df = df[i:i+n*train_length]\n",
    "#         df = df.sample(n*train_length,replace=False)\n",
    "        X.append(np.concatenate(list(df['data'])))\n",
    "        y.extend([f]*df.shape[0])\n",
    "    y_dict = {a:i for i,a in enumerate(np.unique(y))}\n",
    "    y  = [y_dict[a] for a in y]\n",
    "    return np.concatenate(X),np.array(y),y_dict\n",
    "\n",
    "def get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,filepath):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model =  get_model(input_shape=(n_timesteps,n_channels),n_classes=n_classes)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max',save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_acc', mode='max', verbose=0,patience=40)\n",
    "    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=0,factor=0.5)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=300,verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "    print(accuracy_score(val_y,model.predict(val_x).argmax(axis=1)),end=',')\n",
    "    return model\n",
    "\n",
    "def get_model(input_shape=(500,3),n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(128,2,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(GRU(128,return_sequences=False,activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(350,name='feature'))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "window_size = 20\n",
    "activity = 'sports'\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/'\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'\n",
    "min_test_total_length  = 100\n",
    "fs = 25\n",
    "n_timesteps,n_channels = fs*window_size,3 \n",
    "if not os.path.isdir(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "participant_df = get_participants_df(data_directory+'testing/',window_size,min_test_total_length)\n",
    "# n_users = list(np.arange(50,participant_df.shape[0],50))+[participant_df.shape[0]]\n",
    "train_lengths = list(np.arange(10,60,10))+list(np.arange(60,210,30))\n",
    "n_iters = np.arange(3)\n",
    "n_users = [participant_df.shape[0]]\n",
    "# train_lengths = [120]\n",
    "# n_iters = [1]\n",
    "\n",
    "for n_user in n_users[::-1]:\n",
    "    if not os.path.isdir(model_directory+str(n_user)):\n",
    "        os.makedirs(model_directory+str(n_user))\n",
    "    print('--'*30)\n",
    "    print('Starting for no. of training users = ',n_user)\n",
    "    for train_length in train_lengths:\n",
    "        print('Training length minutes = ',train_length)\n",
    "        if not os.path.isdir(model_directory+str(n_user)+'/'+str(train_length)):\n",
    "            os.makedirs(model_directory+str(n_user)+'/'+str(train_length))\n",
    "        for n_iter in n_iters:\n",
    "            X_train,y_train,user_dict = get_training_data(directory = data_directory+'training/',\n",
    "                                                          train_length=train_length,\n",
    "                                                          n_user=n_user,\n",
    "                                                          participant_df=participant_df,\n",
    "                                                          window_size=window_size) \n",
    "            print(X_train.shape)\n",
    "            pickle.dump(user_dict,open(model_directory+str(n_user)+'/'+str(train_length)+'/userdict_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.p','wb'))\n",
    "            model = get_model(input_shape=(n_timesteps,n_channels),n_classes=len(np.unique(y_train)))\n",
    "            model_filepath = model_directory+str(n_user)+'/'+str(train_length)+'/trainedmodel_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.hdf5'\n",
    "            model = get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,model_filepath)\n",
    "            print('iteration = ',n_iter)\n",
    "        print(train_length, '--Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "window_size = 20\n",
    "train_length = 120\n",
    "activity = 'walking'\n",
    "n_user  = 333\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'+str(n_user)+'/'+str(train_length)+'/'\n",
    "\n",
    "model_directory = './models/20/walking/333/120/trained_model_seed_100_iteration_1.hdf5'\n",
    "# model = tf.keras.models.load_model(model_directory+'trained_model_seed_100_iteration_0.hdf5')\n",
    "model = tf.keras.models.load_model(model_directory)\n",
    "\n",
    "# user_dict = pickle.load(open(model_directory+'user_dict_seed_100_iteration_0.p','rb'))\n",
    "\n",
    "user_dict = pickle.load(open('./models/20/walking/333/120/user_dict_seed_100_iteration_1.p','rb'))\n",
    "\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/testing/'\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "def get_training_data(directory,\n",
    "                      test_length,\n",
    "                      user_dict,\n",
    "                      window_size,\n",
    "                      model):\n",
    "    n = 60//window_size\n",
    "    users = list(user_dict.keys())\n",
    "    results = []\n",
    "    y_orig = []\n",
    "    y_pred = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        X = np.concatenate(list(df['data']))\n",
    "        pred = model.predict(X).argmax(axis=1)\n",
    "        results.append(accuracy_score([user_dict[f]]*df.shape[0],pred))\n",
    "        y_orig.extend([user_dict[f]]*df.shape[0])\n",
    "        y_pred.extend(list(pred))\n",
    "    return results,pd.DataFrame({'original':y_orig,'prediction':y_pred})\n",
    "\n",
    "results,df = get_training_data(data_directory,1,user_dict,window_size,model)\n",
    "\n",
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df['original'],df['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('original',as_index=False).apply(lambda a:pd.Series({'accuracy':accuracy_score(a['original'],a['prediction'])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('./models/','zip','./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
