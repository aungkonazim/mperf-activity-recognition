{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,Input,MaxPooling1D,Flatten,Dense,Input,Activation,GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import Parallel,delayed\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[2], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[315]\n",
      "------------------------------------------------------------\n",
      "Starting for no. of training users =  315\n",
      "Training length minutes =  10\n",
      "(9450, 500, 3)\n",
      "0.22486772486772486,iteration =  0\n",
      "10 --Done\n",
      "Training length minutes =  20\n",
      "(18900, 500, 3)\n",
      "0.2619047619047619,iteration =  0\n",
      "20 --Done\n",
      "Training length minutes =  30\n",
      "(28350, 500, 3)\n",
      "0.2784832451499118,iteration =  0\n",
      "30 --Done\n",
      "Training length minutes =  40\n",
      "(37800, 500, 3)\n",
      "0.3193121693121693,iteration =  0\n",
      "40 --Done\n",
      "Training length minutes =  50\n",
      "(47250, 500, 3)\n",
      "0.34645502645502646,iteration =  0\n",
      "50 --Done\n",
      "Training length minutes =  60\n",
      "(56700, 500, 3)\n",
      "0.35343915343915344,iteration =  0\n",
      "60 --Done\n",
      "Training length minutes =  90\n",
      "(85050, 500, 3)\n",
      "0.42580834803057027,iteration =  0\n",
      "90 --Done\n",
      "Training length minutes =  120\n",
      "(113400, 500, 3)\n",
      "0.47063492063492063,iteration =  0\n",
      "120 --Done\n",
      "Training length minutes =  150\n",
      "(141750, 500, 3)\n",
      "0.4831393298059965,iteration =  0\n",
      "150 --Done\n",
      "Training length minutes =  180\n",
      "(170100, 500, 3)\n",
      "0.5034685479129923,iteration =  0\n",
      "180 --Done\n",
      "Training length minutes =  210\n",
      "(198450, 500, 3)\n",
      "0.5273620559334845,iteration =  0\n",
      "210 --Done\n",
      "Training length minutes =  240\n",
      "(226800, 500, 3)\n",
      "0.5351410934744268,iteration =  0\n",
      "240 --Done\n",
      "Training length minutes =  270\n",
      "(246240, 500, 3)\n",
      "0.5395549057829759,iteration =  0\n",
      "270 --Done\n",
      "Training length minutes =  300\n",
      "(270000, 500, 3)\n",
      "0.5524074074074075,iteration =  0\n",
      "300 --Done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def get_participants_df(directory,window_size,min_length):\n",
    "    df = []\n",
    "    n = 60//window_size\n",
    "    for f in os.listdir(directory):\n",
    "        data = pickle.load(open(directory+f,'rb'))\n",
    "        df.append([f,data.shape[0]//n])\n",
    "    df = pd.DataFrame(df,columns=['user','total_test_length'])\n",
    "    return df[df.total_test_length>=min_length]\n",
    "\n",
    "def get_training_data(directory,\n",
    "                      train_length,\n",
    "                      n_user,\n",
    "                      participant_df,\n",
    "                      window_size):\n",
    "    n = 60//window_size\n",
    "    users = participant_df['user'].values[:n_user]\n",
    "    X = []\n",
    "    y = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        if df.shape[0]<=n*train_length:\n",
    "            continue\n",
    "        i = np.random.randint(0,df.shape[0]-n*train_length)\n",
    "        df = df[i:i+n*train_length]\n",
    "#         df = df.sample(n*train_length,replace=False)\n",
    "        X.append(np.concatenate(list(df['data'])))\n",
    "        y.extend([f]*df.shape[0])\n",
    "    y_dict = {a:i for i,a in enumerate(np.unique(y))}\n",
    "    y  = [y_dict[a] for a in y]\n",
    "    return np.concatenate(X),np.array(y),y_dict\n",
    "\n",
    "def get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,filepath):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model =  get_model(input_shape=(n_timesteps,n_channels),n_classes=n_classes)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max',save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_acc', mode='max', verbose=0,patience=40)\n",
    "    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=0,factor=0.5)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=300,verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "    print(accuracy_score(val_y,model.predict(val_x).argmax(axis=1)),end=',')\n",
    "    return model\n",
    "\n",
    "def get_model(input_shape=(500,3),n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(128,2,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(GRU(128,return_sequences=False,activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(350,name='feature'))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "window_size = 20\n",
    "activity = 'std25'\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/'\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'\n",
    "min_test_total_length  = 100\n",
    "fs = 25\n",
    "n_timesteps,n_channels = fs*window_size,3 \n",
    "if not os.path.isdir(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "participant_df = get_participants_df(data_directory+'testing/',window_size,min_test_total_length)\n",
    "# n_users = list(np.arange(50,participant_df.shape[0],50))+[participant_df.shape[0]]\n",
    "n_users = [participant_df.shape[0]]\n",
    "print(n_users)\n",
    "train_lengths = list(np.arange(10,60,10))+list(np.arange(60,330,30))\n",
    "n_iters = np.arange(1)\n",
    "# n_users = np.arange(50,350,50)\n",
    "# train_lengths = [210,240]\n",
    "# n_iters = [1]\n",
    "\n",
    "for n_user in n_users[::-1]:\n",
    "    if not os.path.isdir(model_directory+str(n_user)):\n",
    "        os.makedirs(model_directory+str(n_user))\n",
    "    print('--'*30)\n",
    "    print('Starting for no. of training users = ',n_user)\n",
    "    for train_length in train_lengths:\n",
    "        print('Training length minutes = ',train_length)\n",
    "        if not os.path.isdir(model_directory+str(n_user)+'/'+str(train_length)):\n",
    "            os.makedirs(model_directory+str(n_user)+'/'+str(train_length))\n",
    "        for n_iter in n_iters:\n",
    "            X_train,y_train,user_dict = get_training_data(directory = data_directory+'training/',\n",
    "                                                          train_length=train_length,\n",
    "                                                          n_user=n_user,\n",
    "                                                          participant_df=participant_df,\n",
    "                                                          window_size=window_size) \n",
    "            print(X_train.shape)\n",
    "            pickle.dump(user_dict,open(model_directory+str(n_user)+'/'+str(train_length)+'/userdict_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.p','wb'))\n",
    "            model = get_model(input_shape=(n_timesteps,n_channels),n_classes=len(np.unique(y_train)))\n",
    "            model_filepath = model_directory+str(n_user)+'/'+str(train_length)+'/trainedmodel_seed_'+str(seed)+'_iteration_'+str(n_iter)+'.hdf5'\n",
    "            model = get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,model_filepath)\n",
    "            print('iteration = ',n_iter)\n",
    "        print(train_length, '--Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "window_size = 20\n",
    "train_length = 120\n",
    "activity = 'walking'\n",
    "n_user  = 333\n",
    "model_directory = './models/'+str(window_size)+'/'+activity+'/'+str(n_user)+'/'+str(train_length)+'/'\n",
    "\n",
    "model_directory = './models/20/walking/333/120/trained_model_seed_100_iteration_1.hdf5'\n",
    "# model = tf.keras.models.load_model(model_directory+'trained_model_seed_100_iteration_0.hdf5')\n",
    "model = tf.keras.models.load_model(model_directory)\n",
    "\n",
    "# user_dict = pickle.load(open(model_directory+'user_dict_seed_100_iteration_0.p','rb'))\n",
    "\n",
    "user_dict = pickle.load(open('./models/20/walking/333/120/user_dict_seed_100_iteration_1.p','rb'))\n",
    "\n",
    "data_directory = './data/'+str(window_size)+'/'+activity+'/testing/'\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "def get_training_data(directory,\n",
    "                      test_length,\n",
    "                      user_dict,\n",
    "                      window_size,\n",
    "                      model):\n",
    "    n = 60//window_size\n",
    "    users = list(user_dict.keys())\n",
    "    results = []\n",
    "    y_orig = []\n",
    "    y_pred = []\n",
    "    for f in users:\n",
    "        df = pickle.load(open(directory+f,'rb'))\n",
    "        X = np.concatenate(list(df['data']))\n",
    "        pred = model.predict(X).argmax(axis=1)\n",
    "        results.append(accuracy_score([user_dict[f]]*df.shape[0],pred))\n",
    "        y_orig.extend([user_dict[f]]*df.shape[0])\n",
    "        y_pred.extend(list(pred))\n",
    "    return results,pd.DataFrame({'original':y_orig,'prediction':y_pred})\n",
    "\n",
    "results,df = get_training_data(data_directory,1,user_dict,window_size,model)\n",
    "\n",
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df['original'],df['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('original',as_index=False).apply(lambda a:pd.Series({'accuracy':accuracy_score(a['original'],a['prediction'])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
