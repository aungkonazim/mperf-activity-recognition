{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cerebralcortex/kessel_jupyter_virtualenv/tensorflow/lib/python3.7/site-packages/pandas-1.2.3-py3.7-linux-x86_64.egg/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,Input,MaxPooling1D,Flatten,Dense,Input,Activation,GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import Parallel,delayed\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/10/Sitting/'\n",
    "X,y = [],[]\n",
    "for i,f in enumerate(list(os.listdir(directory))):\n",
    "    if f[0]=='.':\n",
    "        continue\n",
    "    df  = pickle.load(open(directory+f,'rb'))\n",
    "    df['time'] = df['window'].apply(lambda a:a[0])\n",
    "    df['data'] = df['data'].apply(lambda a:np.array([np.array(b) for b in a]))\n",
    "    df['data'] = df['data'].apply(lambda a:a[a[:,0].argsort(),:].reshape(1,-1,4))\n",
    "    df['data'] = df['data'].apply(lambda a:a[:,:,1:])\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "    user_id = df.user.values[0]\n",
    "    X.append(np.concatenate(list(df['data'].values)))\n",
    "    y.append(user_id)\n",
    "#     X_test.append(np.concatenate(list(df['data'].values)))\n",
    "#     y_train.extend([user_id]*X_train[-1].shape[0])\n",
    "#     y_test.extend([user_id]*X_test[-1].shape[0])\n",
    "    del df\n",
    "    print(i,end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/10/Walking/'\n",
    "data_all = []\n",
    "for f in list(os.listdir(directory)):\n",
    "    if f[0]=='.':\n",
    "        continue\n",
    "    data_all.append(pickle.load(open(directory+f,'rb')))\n",
    "data_all[-1].head()  \n",
    "data  = pd.concat(data_all)\n",
    "del data_all\n",
    "unique_users = data.user.unique()\n",
    "unique_users_dict = {u:i for i,u in enumerate(unique_users)}\n",
    "data['user_id'] = data['user'].apply(lambda a:unique_users_dict[a])\n",
    "data['time'] = data['window'].apply(lambda a:a[0])\n",
    "data['data'] = data['data'].apply(lambda a:np.array([np.array(b) for b in a]))\n",
    "data['data'] = data['data'].apply(lambda a:a[a[:,0].argsort(),:].reshape(1,-1,4))\n",
    "data['data'] = data['data'].apply(lambda a:a[:,:,1:])\n",
    "pickle.dump(data,open('./data/10/walking_all_data.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = pickle.load(open('./data/10/sitting_all_data.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/10/sitting/10/50user_dict.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47141/625157785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_user\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_users\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbase_window_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_save_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47141/625157785.py\u001b[0m in \u001b[0;36mget_test_score\u001b[0;34m(window_size, base_window_size, n_user, model_save_path, n_timesteps, n_channels, test_lengths)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique_users_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mmodel_save_path1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_users_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'user_dict.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/10/sitting/10/50user_dict.p'"
     ]
    }
   ],
   "source": [
    "def get_model(input_shape=(500,3),n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(128,2,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Activation('tanh'))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(GRU(128,return_sequences=True,activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_test_scores(model,x_test,y_test,test_size=None,n_iter=1000,base_window_size=20):\n",
    "    def get_score(user,test_size,n_iter,base_window_size):\n",
    "        df = test_df[test_df.original==user]\n",
    "        score1 = []\n",
    "        score2 = []\n",
    "        test_lengths = []\n",
    "        for t in test_size:\n",
    "            n  = np.int64(t*60/base_window_size)\n",
    "            if n>df.shape[0]:\n",
    "                continue\n",
    "            y_true = [df['original'].values[0]]*n_iter\n",
    "            y_pred_majority = []\n",
    "    #         y_pred_maxmean = []\n",
    "            for i in range(n_iter):\n",
    "                temp_df = df.sample(n,replace=False)\n",
    "                y_pred_majority.append(mode(temp_df['prediction'].values)[0][0])\n",
    "    #             temp_df = np.mean(np.concatenate(list(temp_df['probability'])),axis=0)\n",
    "    #             y_pred_maxmean.append(np.argmax(temp_df))\n",
    "\n",
    "\n",
    "            majority_score = accuracy_score(y_true,y_pred_majority)\n",
    "    #         maxmean_score = accuracy_score(y_true,y_pred_maxmean)\n",
    "            score1.append(majority_score)\n",
    "    #         score2.append(maxmean_score)\n",
    "            test_lengths.append(t)\n",
    "        \n",
    "        df = pd.DataFrame({'majority_score':score1,\n",
    "    #                         'maxmean_score':score2,\n",
    "                            'test_lengths':test_lengths})\n",
    "#         print(df.loc[0])\n",
    "        return df\n",
    "    \n",
    "    y_pred = model.predict(x_test).argmax(axis=1)\n",
    "#     y_pred1 = y_pred.argmax(axis=1)\n",
    "    test_df = pd.DataFrame({'prediction':y_pred,'original':y_test})\n",
    "#                             'probability':list(y_pred)})\n",
    "#     test_df['probability'] = test_df['probability'].apply(lambda a:a.reshape(1,-1)) \n",
    "    if test_size is None:\n",
    "        test_size = np.arange(1,120,1)\n",
    "    del x_test\n",
    "    \n",
    "    result  = Parallel(verbose=2,n_jobs=30)(delayed(get_score)(user,test_size,n_iter,base_window_size) for user in np.unique(test_df.original.values))\n",
    "    test_score = pd.concat(result)\n",
    "    acc_ = accuracy_score(test_df['original'],test_df['prediction'])\n",
    "    \n",
    "    test_score = test_score.append(pd.DataFrame({'majority_score':[acc_],\n",
    "#                                                     'maxmean_score':[acc_],\n",
    "                                                    'test_lengths':[0]}))\n",
    "    test_score = test_score.groupby('test_lengths',as_index=False).mean()\n",
    "    return test_score\n",
    "\n",
    "\n",
    "\n",
    "def get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size,filepath):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model =  get_model(input_shape=(n_timesteps,n_channels),n_classes=n_classes)\n",
    "#     model.summary()\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min',save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=40)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "#     train_x,test_x,train_y,test_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=300,verbose=0,\n",
    "          callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "#     y_pred = model.predict(test_x).argmax(axis=1)\n",
    "\n",
    "#     from sklearn.metrics import classification_report\n",
    "\n",
    "#     print(accuracy_score(test_y,y_pred),window_size)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_training_testing(window_size=10,\n",
    "                         base_window_size=10):\n",
    "    n_train = window_size*60//base_window_size\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    def split_data(df):\n",
    "        filepath = directory+df['user'].values[0]\n",
    "        df  = pickle.load(open(filepath,'rb'))\n",
    "        if df.shape[0]<n_train+30*6:\n",
    "            print(filepath[-2:],end=',')\n",
    "            return \n",
    "        df['time'] = df['window'].apply(lambda a:a[0])\n",
    "        df['data'] = df['data'].apply(lambda a:np.array([np.array(b) for b in a]))\n",
    "        df['data'] = df['data'].apply(lambda a:a[a[:,0].argsort(),:].reshape(1,-1,4))\n",
    "        df['data'] = df['data'].apply(lambda a:a[:,:,1:])\n",
    "        df = df.sort_values('time').reset_index(drop=True)\n",
    "        user_id = df.user.values[0]\n",
    "        X_train.append(np.concatenate(list(df[:n_train]['data'].values)))\n",
    "        X_test.append(np.concatenate(list(df[n_train:]['data'].values)))\n",
    "        y_train.extend([user_id]*X_train[-1].shape[0])\n",
    "        y_test.extend([user_id]*X_test[-1].shape[0])\n",
    "        print(filepath[-2:],end=',')\n",
    "        return \n",
    "    data.groupby('user',as_index=False).apply(split_data)\n",
    "    return np.concatenate(X_train),np.concatenate(X_test),y_train,y_test\n",
    "\n",
    "def get_training_testing_sitting(window_size=10,\n",
    "                         base_window_size=10,n_user=300):\n",
    "    n_train = window_size*60//base_window_size\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while i<len(X):\n",
    "        if X[i].shape[0]<n_train+30*6:\n",
    "            i+=1\n",
    "            continue\n",
    "        X_train.append(X[i][:n_train])\n",
    "        X_test.append(X[i][n_train:])\n",
    "        y_train.extend([y[i]]*X_train[-1].shape[0])\n",
    "        y_test.extend([y[i]]*X_test[-1].shape[0])\n",
    "        count+=1\n",
    "        i+=1\n",
    "        if count==n_user:\n",
    "            break\n",
    "    return np.concatenate(X_train),np.concatenate(X_test),y_train,y_test\n",
    "\n",
    "\n",
    "# directory = './data/walking_10/right_wrist/'\n",
    "# filepath1 = './data/all_walking_data_10.p'\n",
    "# if not os.path.isfile(filepath1):\n",
    "#     print(filepath1)\n",
    "#     data_col = []\n",
    "#     for f in os.listdir(directory):\n",
    "#         filepath = directory+f\n",
    "#         data  = pickle.load(open(filepath,'rb'))\n",
    "#         data_col.append(df)\n",
    "#     data = pd.concat(data_col)\n",
    "#     unique_users = data.user.unique()\n",
    "#     unique_users_dict = {u:i for i,u in enumerate(unique_users)}\n",
    "#     data['user_id'] = data['user'].apply(lambda a:unique_users_dict[a])\n",
    "#     data['time'] = data['window'].apply(lambda a:a[0])\n",
    "#     data['data'] = data['data'].apply(lambda a:np.array([np.array(b) for b in a]))\n",
    "#     data['data'] = data['data'].apply(lambda a:a[a[:,0].argsort(),:].reshape(1,-1,4))\n",
    "#     data['data'] = data['data'].apply(lambda a:a[:,:,1:])\n",
    "#     pickle.dump(data,open(filepath1,'wb'))\n",
    "# data = pd.DataFrame({'user':list(os.listdir(directory))})\n",
    "\n",
    "\n",
    "def get_test_score(window_size,base_window_size,n_user,model_save_path,n_timesteps,n_channels,test_lengths):\n",
    "    X_train,X_test,y_train,y_test = get_training_testing_sitting(window_size=window_size,base_window_size=base_window_size,n_user=n_user)\n",
    "    unique_users = np.unique(y_train+y_test)\n",
    "    unique_users_dict = {u:i for i,u in enumerate(unique_users)}\n",
    "    y_train = np.array([unique_users_dict[a] for a in y_train])\n",
    "    y_test = np.array([unique_users_dict[a] for a in y_test])\n",
    "    model_save_path1 = model_save_path+ str(window_size)+'/'\n",
    "    pickle.dump(unique_users_dict,open(model_save_path1+str(n_user)+'user_dict.p','wb'))\n",
    "    if not os.path.isdir(model_save_path):\n",
    "        os.makedirs(model_save_path)        \n",
    "    filepath = model_save_path1+str(n_user)+'persons.hdf5'\n",
    "    model = get_trained_model(np.nan_to_num(X_train),y_train,n_timesteps,n_channels,window_size,filepath=filepath)\n",
    "    test_score = get_test_scores(model,np.nan_to_num(X_test),y_test,test_size=test_lengths,n_iter=1000,base_window_size=base_window_size)\n",
    "    test_score['train_user'] = n_user\n",
    "    test_score['train_lengths'] = window_size\n",
    "    del X_train\n",
    "    del X_test\n",
    "    del y_train\n",
    "    del y_test\n",
    "    return test_score\n",
    "\n",
    "\n",
    "base_window_size = 10\n",
    "Fs = 25\n",
    "n_timesteps = base_window_size*Fs\n",
    "n_channels = 3\n",
    "window_sizes = [10,20,40,50]+list(np.arange(30,330,30))\n",
    "test_lengths = [10,20] + list(np.arange(0,330,30)[1:])\n",
    "n_users = list(range(50,350,50))\n",
    "model_save_path = './models/'+str(base_window_size)+'/sitting/'\n",
    "result_save_path = './results/sitting/'\n",
    "if not os.path.isdir(result_save_path):\n",
    "    os.makedirs(result_save_path)\n",
    "if not os.path.isdir(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "results = []\n",
    "for window_size in window_sizes:\n",
    "    results = []\n",
    "    for n_user in n_users:\n",
    "        test_score = get_test_score(window_size,base_window_size,n_user,model_save_path,n_timesteps,n_channels,test_lengths)\n",
    "        results.append(test_score)\n",
    "        pickle.dump(pd.concat(results),open(result_save_path+str(window_size)+'.p','wb'))\n",
    "        print(n_user,end=',')\n",
    "    print(window_size,'done','-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
