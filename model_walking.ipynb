{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,Input,MaxPooling1D,Flatten,Dense,Input,Activation,GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import Parallel,delayed\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/10/Sitting/'\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for f in list(os.listdir(directory)):\n",
    "    if f[0]=='.':\n",
    "        continue\n",
    "    df  = pickle.load(open(directory+f,'rb'))\n",
    "    df['time'] = df['window'].apply(lambda a:a[0])\n",
    "    df['data'] = df['data'].apply(lambda a:np.array([np.array(b) for b in a]))\n",
    "    df['data'] = df['data'].apply(lambda a:a[a[:,0].argsort(),:].reshape(1,-1,4))\n",
    "    df['data'] = df['data'].apply(lambda a:a[:,:,1:])\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "    user_id = df.user.values[0]\n",
    "    X_train.append(np.concatenate(list(df['data'].values)))\n",
    "    X_test.append(np.concatenate(list(df['data'].values)))\n",
    "    y_train.extend([user_id]*X_train[-1].shape[0])\n",
    "    y_test.extend([user_id]*X_test[-1].shape[0])\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = np.concatenate(X_train),np.concatenate(X_test),y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/10/Sitting/'\n",
    "data_all = []\n",
    "for f in list(os.listdir(directory))[:200]:\n",
    "    if f[0]=='.':\n",
    "        continue\n",
    "    data_all.append(pickle.load(open(directory+f,'rb')))\n",
    "data_all[-1].head()  \n",
    "data  = pd.concat(data_all)\n",
    "del data_all\n",
    "unique_users = data.user.unique()\n",
    "unique_users_dict = {u:i for i,u in enumerate(unique_users)}\n",
    "data['user_id'] = data['user'].apply(lambda a:unique_users_dict[a])\n",
    "data['time'] = data['window'].apply(lambda a:a[0])\n",
    "data['data'] = data['data'].apply(lambda a:np.array([np.array(b) for b in a]))\n",
    "data['data'] = data['data'].apply(lambda a:a[a[:,0].argsort(),:].reshape(1,-1,4))\n",
    "data['data'] = data['data'].apply(lambda a:a[:,:,1:])\n",
    "\n",
    "pickle.dump(data,open('./data/10/sitting_all_data.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape=(500,3),n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(128,2,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Activation('tanh'))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(GRU(128,return_sequences=True,activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_score(df,test_size,n_iter,base_window_size):\n",
    "    score1 = []\n",
    "    score2 = []\n",
    "    test_lengths = []\n",
    "    for t in test_size:\n",
    "        n  = np.int64(t*60/base_window_size)\n",
    "        if n>df.shape[0]:\n",
    "            continue\n",
    "        y_true = [df['original'].values[0]]*n_iter\n",
    "        y_pred_majority = []\n",
    "        y_pred_maxmean = []\n",
    "        for i in range(n_iter):\n",
    "            temp_df = df.sample(n,replace=False)\n",
    "            y_pred_majority.append(mode(temp_df['prediction'].values)[0][0])\n",
    "            temp_df = np.mean(np.concatenate(list(temp_df['probability'])),axis=0)\n",
    "            y_pred_maxmean.append(np.argmax(temp_df))\n",
    "        \n",
    "            \n",
    "        majority_score = accuracy_score(y_true,y_pred_majority)\n",
    "        maxmean_score = accuracy_score(y_true,y_pred_maxmean)\n",
    "        score1.append(majority_score)\n",
    "        score2.append(maxmean_score)\n",
    "        test_lengths.append(t)\n",
    "    return pd.DataFrame({'user':[df['original'].values[0]]*len(score1),\n",
    "                        'majority_score':score1,\n",
    "                        'maxmean_score':score2,\n",
    "                        'test_lengths':test_lengths})\n",
    "\n",
    "\n",
    "def get_test_scores(model,X_test,y_test,test_size=None,n_iter=1000,base_window_size=20):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred1 = y_pred.argmax(axis=1)\n",
    "    test_df = pd.DataFrame({'prediction':y_pred1,'original':y_test,'probability':list(y_pred)})\n",
    "    test_df['probability'] = test_df['probability'].apply(lambda a:a.reshape(1,-1)) \n",
    "    if test_size is None:\n",
    "        test_size = np.arange(1,120,1)\n",
    "    result  = Parallel(n_jobs=-1,verbose=2)(delayed(get_score)(df,test_size,n_iter,base_window_size) for i,df in test_df.groupby('original',as_index=False))\n",
    "    test_score = pd.concat(result)\n",
    "    return test_score,test_df\n",
    "\n",
    "\n",
    "\n",
    "def get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model =  get_model(input_shape=(n_timesteps,n_channels),n_classes=n_classes)\n",
    "    model.summary()\n",
    "    filepath = './models/walking/person_estimator_mperf_base_10_seconds_train_'+str(window_size)+'_seconds.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min',save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=40)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "#     train_x,test_x,train_y,test_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=300,verbose=0,\n",
    "          callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "#     y_pred = model.predict(test_x).argmax(axis=1)\n",
    "\n",
    "#     from sklearn.metrics import classification_report\n",
    "\n",
    "#     print(accuracy_score(test_y,y_pred),window_size)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_training_testing(window_size=10,\n",
    "                         base_window_size=20):\n",
    "    n_train = window_size*60//base_window_size\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    def split_data(df):\n",
    "        filepath = directory+df['user'].values[0]\n",
    "        df  = pickle.load(open(filepath,'rb'))\n",
    "        if df.shape[0]<n_train+30*6:\n",
    "            print(filepath[-2:],end=',')\n",
    "            return \n",
    "        df['time'] = df['window'].apply(lambda a:a[0])\n",
    "        df['data'] = df['data'].apply(lambda a:np.array([np.array(b) for b in a]))\n",
    "        df['data'] = df['data'].apply(lambda a:a[a[:,0].argsort(),:].reshape(1,-1,4))\n",
    "        df['data'] = df['data'].apply(lambda a:a[:,:,1:])\n",
    "        df = df.sort_values('time').reset_index(drop=True)\n",
    "        user_id = df.user.values[0]\n",
    "        X_train.append(np.concatenate(list(df[:n_train]['data'].values)))\n",
    "        X_test.append(np.concatenate(list(df[n_train:]['data'].values)))\n",
    "        y_train.extend([user_id]*X_train[-1].shape[0])\n",
    "        y_test.extend([user_id]*X_test[-1].shape[0])\n",
    "        print(filepath[-2:],end=',')\n",
    "        return \n",
    "    data.groupby('user',as_index=False).apply(split_data)\n",
    "    return np.concatenate(X_train),np.concatenate(X_test),y_train,y_test\n",
    "\n",
    "\n",
    "directory = './data/walking_10/right_wrist/'\n",
    "# filepath1 = './data/all_walking_data_10.p'\n",
    "# if not os.path.isfile(filepath1):\n",
    "#     print(filepath1)\n",
    "#     data_col = []\n",
    "#     for f in os.listdir(directory):\n",
    "#         filepath = directory+f\n",
    "#         data  = pickle.load(open(filepath,'rb'))\n",
    "#         data_col.append(df)\n",
    "#     data = pd.concat(data_col)\n",
    "#     unique_users = data.user.unique()\n",
    "#     unique_users_dict = {u:i for i,u in enumerate(unique_users)}\n",
    "#     data['user_id'] = data['user'].apply(lambda a:unique_users_dict[a])\n",
    "#     data['time'] = data['window'].apply(lambda a:a[0])\n",
    "#     data['data'] = data['data'].apply(lambda a:np.array([np.array(b) for b in a]))\n",
    "#     data['data'] = data['data'].apply(lambda a:a[a[:,0].argsort(),:].reshape(1,-1,4))\n",
    "#     data['data'] = data['data'].apply(lambda a:a[:,:,1:])\n",
    "#     pickle.dump(data,open(filepath1,'wb'))\n",
    "data = pd.DataFrame({'user':list(os.listdir(directory))})\n",
    "\n",
    "base_window_size = 10\n",
    "Fs = 25\n",
    "n_timesteps = base_window_size*Fs\n",
    "n_channels = 3\n",
    "window_sizes = np.arange(10,250,10)\n",
    "test_lengths = np.arange(1,180,2)\n",
    "results = []\n",
    "for window_size in window_sizes:\n",
    "#     data = pickle.load(open(filepath1,'rb'))\n",
    "    X_train,X_test,y_train,y_test = get_training_testing(window_size=window_size,base_window_size=base_window_size)\n",
    "    unique_users = np.unique(y_train+y_test)\n",
    "    unique_users_dict = {u:i for i,u in enumerate(unique_users)}\n",
    "    y_train = np.array([unique_users_dict[a] for a in y_train])\n",
    "    y_test = np.array([unique_users_dict[a] for a in y_test])\n",
    "    print(X_train.shape,X_test.shape,len(np.unique(y_train)),len(np.unique(y_test)))\n",
    "    model = get_trained_model(X_train,y_train,n_timesteps,n_channels,window_size)\n",
    "    test_score,test_df = get_test_scores(model,X_test,y_test,test_size=test_lengths,n_iter=1000,base_window_size=20)\n",
    "    results.append([window_size,test_score,test_df,y_test,unique_users_dict])\n",
    "    pickle.dump(results,open('./data/walking_results_10.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score.groupby('test_lengths').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/walking_10/right_wrist/'\n",
    "filepath1 = './data/all_walking_data_10.p'\n",
    "if not os.path.isfile(filepath1):\n",
    "    print(filepath1)\n",
    "    data_col = []\n",
    "    for i,f in enumerate(list(os.listdir(directory))):\n",
    "        filepath = directory+f\n",
    "        df  = pickle.load(open(filepath,'rb'))\n",
    "        data_col.append(df)\n",
    "        print(i,end=',')\n",
    "    data_col = pd.concat(data_col)\n",
    "    unique_users = data.user.unique()\n",
    "    unique_users_dict = {u:i for i,u in enumerate(unique_users)}\n",
    "    data['user_id'] = data['user'].apply(lambda a:unique_users_dict[a])\n",
    "    data['time'] = data['window'].apply(lambda a:a[0])\n",
    "    data['data'] = data['data'].apply(lambda a:np.array([np.array(b) for b in a]))\n",
    "    data['data'] = data['data'].apply(lambda a:a[a[:,0].argsort(),:].reshape(1,-1,4))\n",
    "    data['data'] = data['data'].apply(lambda a:a[:,:,1:])\n",
    "    pickle.dump(data,open(filepath1,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score.groupby('test_lengths').mean()[['maxmean_score','majority_score']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
